{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Retrieval-Augmented Generation"
      ],
      "metadata": {
        "id": "C-VBSSUjQCYR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setting up the LLM"
      ],
      "metadata": {
        "id": "ZWK-qrtRn-v-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade langchain\n",
        "!pip install --upgrade langchain-core\n",
        "!pip install --upgrade langchain-community\n",
        "!pip install --upgrade langchain-google-genai"
      ],
      "metadata": {
        "id": "HVK_hO-EU99g",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### First Test"
      ],
      "metadata": {
        "id": "-F08gJvPoQhX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain_core.messages import HumanMessage, SystemMessage\n",
        "\n",
        "# 1. Initialize the Chat Model (from the previous step)\n",
        "chat_model = ChatGoogleGenerativeAI(model=\"gemini-2.5-flash\",\n",
        "                                  temperature=0,\n",
        "                                  google_api_key=userdata.get('google_api_key'))\n",
        "\n",
        "# 2. Prepare the messages\n",
        "# The SystemMessage sets the behavior and context for the AI.\n",
        "# The HumanMessage is the user's actual query.\n",
        "messages = [\n",
        "    SystemMessage(content=\"You're an assistant knowledgeable about healthcare. Only answer healthcare-related questions.\"),\n",
        "    HumanMessage(content=\"What is Ayushman Bharat?\"),\n",
        "]\n",
        "\n",
        "# 3. Invoke the model with the messages\n",
        "result = chat_model.invoke(messages) # notice the similarity with model.predict from sklearn\n",
        "\n",
        "print(result.content)"
      ],
      "metadata": {
        "id": "4N2YE7g0UVqd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "add2c307-eb15-4d00-da7f-3a0f68a7d7a2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ayushman Bharat is a flagship national health protection scheme launched by the Government of India in 2018. Its primary goal is to achieve Universal Health Coverage (UHC) and ensure that no one is left behind due to financial constraints when seeking healthcare.\n",
            "\n",
            "The scheme has two main components:\n",
            "\n",
            "1.  **Pradhan Mantri Jan Arogya Yojana (PMJAY):** This is the health insurance component, often referred to as the world's largest government-funded health insurance scheme.\n",
            "    *   **Objective:** To provide financial protection to over 50 crore (500 million) poor and vulnerable families for secondary and tertiary care hospitalization.\n",
            "    *   **Coverage:** It offers a health cover of up to â‚¹5 lakh (approximately $6,000 USD) per family per year for cashless and paperless treatment at empanelled public and private hospitals across India.\n",
            "    *   **Benefits:** Covers pre-hospitalization expenses, hospitalization expenses, and post-hospitalization expenses for a wide range of medical and surgical procedures.\n",
            "    *   **Eligibility:** Based on the Socio-Economic Caste Census (SECC) 2011 data, targeting specific deprivation and occupational criteria in rural and urban areas, respectively.\n",
            "\n",
            "2.  **Ayushman Bharat Health and Wellness Centres (AB-HWCs):** This component aims to transform existing Sub Centres and Primary Health Centres into Health and Wellness Centres.\n",
            "    *   **Objective:** To bring healthcare closer to the homes of people, focusing on comprehensive primary healthcare.\n",
            "    *   **Services:** They provide a wide range of services, including:\n",
            "        *   Maternal and child health services.\n",
            "        *   Non-communicable diseases (NCDs) screening and management (e.g., hypertension, diabetes, common cancers).\n",
            "        *   Care for the elderly.\n",
            "        *   Mental health services.\n",
            "        *   Oral health.\n",
            "        *   Eye care.\n",
            "        *   Emergency medical services.\n",
            "        *   Yoga and wellness activities.\n",
            "    *   **Focus:** Emphasizes preventive, promotive, curative, palliative, and rehabilitative care, moving beyond selective care to comprehensive care.\n",
            "\n",
            "In essence, Ayushman Bharat is a two-pronged approach to healthcare reform in India, addressing both the financial burden of hospitalization through PMJAY and strengthening primary healthcare delivery through AB-HWCs.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chat_model.invoke(\"What is blood pressure?\")"
      ],
      "metadata": {
        "id": "QzzuvFGqUn0k",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "74f5e6ba-8131-4cff-9151-d88a63da7ca7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='Blood pressure is the **force of your blood pushing against the walls of your arteries** as your heart pumps it throughout your body.\\n\\nThink of your heart as a pump and your arteries as a network of hoses. When the pump pushes blood, it creates pressure inside the hoses. This pressure is essential to deliver oxygen and nutrients to all your organs and tissues.\\n\\n**How it\\'s Measured (The Two Numbers):**\\n\\nBlood pressure is expressed as two numbers, typically written as a fraction (e.g., 120/80 mmHg):\\n\\n1.  **Systolic Pressure (the top number):** This is the higher number and represents the pressure in your arteries when your heart **beats** (contracts) and pushes blood out.\\n2.  **Diastolic Pressure (the bottom number):** This is the lower number and represents the pressure in your arteries when your heart is **at rest** (between beats) and filling with blood.\\n\\nThe unit \"mmHg\" stands for millimeters of mercury, which is the standard unit for measuring pressure in this context.\\n\\n**What the Numbers Mean (General Guidelines for Adults):**\\n\\n*   **Normal:** Less than 120/80 mmHg\\n*   **Elevated:** Systolic between 120-129 and diastolic less than 80\\n*   **High Blood Pressure (Hypertension) Stage 1:** Systolic between 130-139 or diastolic between 80-89\\n*   **High Blood Pressure (Hypertension) Stage 2:** Systolic 140 or higher or diastolic 90 or higher\\n*   **Hypertensive Crisis:** Systolic higher than 180 and/or diastolic higher than 120 (requires immediate medical attention)\\n\\n**Why is Blood Pressure Important?**\\n\\n*   **Too Low (Hypotension):** If your blood pressure is too low, your organs might not get enough blood, leading to symptoms like dizziness, lightheadedness, fainting, or even shock in severe cases.\\n*   **Too High (Hypertension):** This is often called the \"silent killer\" because it usually has no symptoms. Over time, high blood pressure can damage your arteries, heart, brain, kidneys, and eyes. It significantly increases your risk of serious health problems like:\\n    *   Heart attack\\n    *   Stroke\\n    *   Heart failure\\n    *   Kidney disease\\n    *   Vision loss\\n\\nRegular blood pressure checks are crucial because they can detect problems early, allowing for lifestyle changes or medication to manage it and prevent severe complications.', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash', 'safety_ratings': []}, id='run--22e12d4d-36b3-478e-a0c1-2f1fae87df10-0', usage_metadata={'input_tokens': 6, 'output_tokens': 1637, 'total_tokens': 1643, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 1093}})"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Second Test"
      ],
      "metadata": {
        "id": "FmD3qL7_oTGd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain_core.messages import HumanMessage, SystemMessage\n",
        "\n",
        "# 1. Initialize the Chat Model\n",
        "chat_model = ChatGoogleGenerativeAI(model=\"gemini-2.5-flash\",\n",
        "                                  temperature=0,\n",
        "                                  google_api_key=userdata.get('google_api_key'))\n",
        "\n",
        "# 2. Prepare messages with an out-of-scope question\n",
        "messages = [\n",
        "    SystemMessage(content=\"You're an assistant knowledgeable about healthcare. Only answer healthcare-related questions.\"),\n",
        "    HumanMessage(content=\"How do I change a tire?\"),\n",
        "]\n",
        "\n",
        "# 3. Invoke the model\n",
        "result = chat_model.invoke(messages)\n",
        "\n",
        "print(result.content)"
      ],
      "metadata": {
        "id": "y_eeP53XUFLr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7c8b0c7e-e744-4052-98f0-30056a68519c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I cannot provide instructions on how to change a tire.  That's not within the scope of healthcare advice.  Changing a tire is a mechanical task and requires a separate set of instructions.  If you need to change a tire, I recommend consulting a trusted automotive resource or calling for roadside assistance.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ChatPromptTemplate"
      ],
      "metadata": {
        "id": "P53LlfJSpQ-I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "# 1. Initialize the Chat Model\n",
        "chat_model = ChatGoogleGenerativeAI(model=\"gemini-2.5-flash\",\n",
        "                                  temperature=0,\n",
        "                                  google_api_key=userdata.get('google_api_key'))\n",
        "\n",
        "# 2. Create the Prompt Template\n",
        "instruction_str = \"\"\"Your job is to use patient reviews to answer questions about their experience at a hospital.\n",
        "Use the following context to answer questions. Be as detailed as possible, but don't make up any information that's not from the context.\n",
        "If you don't know an answer, say you don't know.\n",
        "\n",
        "Context: {context}\n",
        "\n",
        "Question: {question}\n",
        "\"\"\"\n",
        "\n",
        "review_template = ChatPromptTemplate.from_template(instruction_str)\n",
        "\n",
        "# 3. Define the context and question\n",
        "context = \"The discharge process was seamless!\"\n",
        "question = \"Did anyone have a positive experience?\"\n",
        "\n",
        "# 4. Create the chain by piping the components together\n",
        "#    We also add an output parser to get a clean string result.\n",
        "chain = review_template | chat_model | StrOutputParser()\n",
        "\n",
        "# 5. Invoke the chain with the input variables\n",
        "result = chain.invoke({\n",
        "    \"context\": context,\n",
        "    \"question\": question\n",
        "})\n",
        "\n",
        "print(result)"
      ],
      "metadata": {
        "id": "pNAWRLXoTePQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3d563d4a-49a5-4016-ad8f-61f5b46661db"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Yes, at least one patient had a positive experience.  One review specifically mentions that the discharge process was seamless.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Using PromptTemplates & MessageTemplates"
      ],
      "metadata": {
        "id": "--bZZ51YYb0s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain_core.prompts import (\n",
        "    PromptTemplate,\n",
        "    SystemMessagePromptTemplate,\n",
        "    HumanMessagePromptTemplate,\n",
        "    ChatPromptTemplate\n",
        ")\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "# 1. Initialize the Chat Model\n",
        "chat_model = ChatGoogleGenerativeAI(model=\"gemini-2.5-flash\",\n",
        "                                  temperature=0,\n",
        "                                  google_api_key=userdata.get('google_api_key'))\n",
        "\n",
        "# 2. Create the detailed prompt templates\n",
        "instruction_str = \"\"\"Your job is to use patient reviews to answer questions about their experience at a hospital.\n",
        "Use the following context to answer questions.\n",
        "Be as detailed as possible, but don't make up any information that's not from the context.\n",
        "If you don't know an answer, say you don't know.\n",
        "\n",
        "Context: {context}\n",
        "\"\"\"\n",
        "\n",
        "review_system_prompt = SystemMessagePromptTemplate(\n",
        "    prompt=PromptTemplate(\n",
        "        input_variables=[\"context\"], template=instruction_str\n",
        "    )\n",
        ")\n",
        "\n",
        "review_human_prompt = HumanMessagePromptTemplate(\n",
        "    prompt=PromptTemplate(\n",
        "        input_variables=[\"question\"], template=\"{question}\"\n",
        "    )\n",
        ")\n",
        "\n",
        "messages = [review_system_prompt, review_human_prompt]\n",
        "\n",
        "# This is our final, reusable prompt template\n",
        "review_prompt_template = ChatPromptTemplate(\n",
        "    input_variables=[\"context\", \"question\"],\n",
        "    messages=messages,\n",
        ")\n",
        "\n",
        "# 3. Define the context and question\n",
        "context = \"I had a great stay!\"\n",
        "question = \"Did anyone have a positive experience?\"\n",
        "\n",
        "# 4. Create the chain\n",
        "chain = review_prompt_template | chat_model | StrOutputParser()\n",
        "\n",
        "# 5. Invoke the chain\n",
        "result = chain.invoke({\n",
        "    \"context\": context,\n",
        "    \"question\": question\n",
        "})\n",
        "\n",
        "print(result)"
      ],
      "metadata": {
        "id": "GhrsnDNPTeMa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ee9d7682-181b-4a64-e507-e617b9bc84b7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Yes, one patient stated, \"I had a great stay!\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "context = \"I had a negative stay!\"\n",
        "question = \"Did anyone have a positive experience?\"\n",
        "\n",
        "chain.invoke({\"context\": context, \"question\": question})"
      ],
      "metadata": {
        "id": "hiKXXNE9Td0i",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "outputId": "28d4f60a-5d59-4a51-cb8e-e6efd673e09c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Based on the provided context, it only states, \"I had a negative stay!\" There is no information about anyone having a positive experience.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Adding RAG"
      ],
      "metadata": {
        "id": "3KH3JCq5ra_2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U langchain-chroma # FAISS"
      ],
      "metadata": {
        "collapsed": true,
        "id": "OD9upnVJkxDV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "from google.colab import userdata\n",
        "\n",
        "# Import the CSVLoader class to load documents from a CSV file.\n",
        "from langchain.document_loaders.csv_loader import CSVLoader\n",
        "\n",
        "# Import the Chroma class, which is used to create and interact with a Chroma vector database.\n",
        "from langchain_chroma import Chroma\n",
        "\n",
        "# Import the GoogleGenerativeAIEmbeddings class to create numerical vector representations (embeddings) of text using Google's models.\n",
        "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
        "\n",
        "\n",
        "REVIEWS_CSV_PATH = \"reviews.csv\"\n",
        "\n",
        "# Define a constant variable for the directory where the Chroma vector database will be stored.\n",
        "REVIEWS_CHROMA_PATH = \"chroma_data\"\n",
        "\n",
        "\n",
        "# Create an instance of the CSVLoader.\n",
        "loader = CSVLoader(\n",
        "    file_path=REVIEWS_CSV_PATH,  # Specify the path to the CSV file to be loaded.\n",
        "    source_column=\"review\"       # Specify the name of the column that contains the main text content.\n",
        ")\n",
        "\n",
        "# Call the .load() method on the loader instance.\n",
        "# This reads the specified column from the CSV file and loads the content into a list of Document objects.\n",
        "reviews = loader.load()\n",
        "\n",
        "# Specify the embedding function to use. We define it once to be reused.\n",
        "embedding_function = GoogleGenerativeAIEmbeddings(\n",
        "    model=\"models/gemini-embedding-001\",  # Choose the specific embedding model provided by Google.\n",
        "    google_api_key=userdata.get('GOOGLE_API_KEY')  # Securely fetch the Google API key.\n",
        ")\n",
        "\n",
        "# Set the size of each batch to process.\n",
        "batch_size = 20\n",
        "# Calculate the total number of batches.\n",
        "num_batches = (len(reviews) - 1) // batch_size + 1\n",
        "reviews_vector_db = None\n",
        "\n",
        "# Loop through the documents in batches to avoid hitting the API's rate limit.\n",
        "for i in range(0, len(reviews), batch_size):\n",
        "    # Get the current batch of documents.\n",
        "    batch_docs = reviews[i:i + batch_size]\n",
        "    current_batch_num = i // batch_size + 1\n",
        "\n",
        "    print(f\"Processing batch {current_batch_num}/{num_batches}...\")\n",
        "\n",
        "    if i == 0:\n",
        "        # For the first batch, create a new Chroma vector database.\n",
        "        # The `from_documents` method handles the entire process of embedding and storing the data.\n",
        "        reviews_vector_db = Chroma.from_documents(\n",
        "            documents=batch_docs,  # Pass the list of Document objects that need to be embedded.\n",
        "            embedding=embedding_function,\n",
        "            # Specify the directory on the disk where the vector database will be saved.\n",
        "            # This makes the database persistent, so we can load it directly in the future.\n",
        "            persist_directory=REVIEWS_CHROMA_PATH\n",
        "        )\n",
        "    else:\n",
        "        # For subsequent batches, add the documents to the existing database.\n",
        "        reviews_vector_db.add_documents(documents=batch_docs)\n",
        "\n",
        "    # Pause the script for 30 seconds after each batch to respect the per-minute rate limit.\n",
        "    print(f\"Batch {current_batch_num} processed. Waiting for 30 seconds...\")\n",
        "    time.sleep(30)\n",
        "\n",
        "print(\"Vector database created successfully and saved to the specified directory.\")"
      ],
      "metadata": {
        "id": "8vrBwvI9YwTr",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 523
        },
        "outputId": "4ed6f2d2-0160-4dac-c8b1-de029ae85b8d"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 1/51...\n",
            "Batch 1 processed. Waiting for 30 seconds...\n",
            "Processing batch 2/51...\n",
            "Batch 2 processed. Waiting for 30 seconds...\n",
            "Processing batch 3/51...\n",
            "Batch 3 processed. Waiting for 30 seconds...\n",
            "Processing batch 4/51...\n",
            "Batch 4 processed. Waiting for 30 seconds...\n",
            "Processing batch 5/51...\n",
            "Batch 5 processed. Waiting for 30 seconds...\n",
            "Processing batch 6/51...\n",
            "Batch 6 processed. Waiting for 30 seconds...\n",
            "Processing batch 7/51...\n",
            "Batch 7 processed. Waiting for 30 seconds...\n",
            "Processing batch 8/51...\n",
            "Batch 8 processed. Waiting for 30 seconds...\n",
            "Processing batch 9/51...\n",
            "Batch 9 processed. Waiting for 30 seconds...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1197790138.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     64\u001b[0m     \u001b[0;31m# Pause the script for 30 seconds after each batch to respect the per-minute rate limit.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Batch {current_batch_num} processed. Waiting for 30 seconds...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m     \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Vector database created successfully and saved to the specified directory.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Retrieval"
      ],
      "metadata": {
        "id": "M0082YQZi7dG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"\"\"Has anyone complained about communication with the hospital staff?\"\"\"\n",
        "relevant_chunks = reviews_vector_db.similarity_search(question, k=3)\n",
        "\n",
        "relevant_chunks[0].page_content"
      ],
      "metadata": {
        "id": "FEKxBKTNi7LZ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "outputId": "c65b11d6-5319-4e25-d92a-5f3cf5e4d348"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"review_id: 707\\nvisit_id: 4533\\nreview: I encountered some issues with the nursing staff's communication. It seemed like there was a lack of coordination, leading to confusion about my medication schedule and treatment plan.\\nphysician_name: Joseph Gonzales\\nhospital_name: Brown-Golden\\npatient_name: Makayla Reynolds\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "relevant_chunks[1].page_content"
      ],
      "metadata": {
        "id": "qrmyucMmffPJ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "outputId": "82ae9317-7c34-49f0-d73c-02323efe4db3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"review_id: 707\\nvisit_id: 4533\\nreview: I encountered some issues with the nursing staff's communication. It seemed like there was a lack of coordination, leading to confusion about my medication schedule and treatment plan.\\nphysician_name: Joseph Gonzales\\nhospital_name: Brown-Golden\\npatient_name: Makayla Reynolds\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "relevant_chunks[2].page_content"
      ],
      "metadata": {
        "id": "NeYvAtnSjATn",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "outputId": "314151a9-2b01-4745-d396-400e1e2fcdb7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'review_id: 43\\nvisit_id: 2353\\nreview: The hospital staff lacked proper communication among themselves, leading to confusion about my treatment plan. Clear and cohesive communication is essential for patient care and overall satisfaction.\\nphysician_name: Erika Ingram\\nhospital_name: Shea LLC\\npatient_name: Dennis Fitzgerald'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.schema.runnable import RunnablePassthrough  # Allows passing inputs through unchanged in a pipeline\n",
        "from langchain_core.output_parsers import StrOutputParser  # Parses the model's output into a clean string\n",
        "\n",
        "# Create a retriever to fetch the top 10 most relevant reviews based on a query\n",
        "reviews_retriever = reviews_vector_db.as_retriever(k=10)\n",
        "# The `as_retriever` method converts the database into a retriever.\n",
        "# `k=10` specifies that the retriever should return the top 10 most relevant documents for a query.\n",
        "\n",
        "# Create a chain for querying and generating responses\n",
        "review_chain = (\n",
        "    {\"context\": reviews_retriever, \"question\": RunnablePassthrough()}\n",
        "    # Step 1: Retrieves relevant reviews (`context`) and passes the `question` unchanged\n",
        "    | review_prompt_template\n",
        "    # Step 2: Formats the retrieved reviews and the user's question into a structured prompt\n",
        "    | chat_model\n",
        "    # Step 3: Sends the prompt to the Gemini chat model to generate a response\n",
        "    | StrOutputParser()\n",
        "    # Step 4: Parses the model's raw output into a clean string format for easier use\n",
        ")"
      ],
      "metadata": {
        "id": "L_-CkirwquWu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"\"\"Has anyone complained about communication with the hospital staff?\"\"\"\n",
        "review_chain.invoke(question)"
      ],
      "metadata": {
        "id": "tkRCoPB5ro7_",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 142
        },
        "outputId": "fe6dd871-1bdc-45f5-b62f-f8689adec7a1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Yes, patients have complained about communication with the hospital staff.\\n\\n*   **Makayla Reynolds** (review_id: 707) encountered issues with the nursing staff's communication at Brown-Golden hospital, noting a lack of coordination that led to confusion about her medication schedule and treatment plan.\\n*   **Dennis Fitzgerald** (review_id: 43) stated that the hospital staff at Shea LLC lacked proper communication among themselves, which caused confusion about his treatment plan. He emphasized that clear and cohesive communication is essential for patient care and overall satisfaction.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Putting it all together"
      ],
      "metadata": {
        "id": "0FG3EW1-laiD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain_core.messages import HumanMessage, SystemMessage"
      ],
      "metadata": {
        "id": "p62iji89p1nj"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import (\n",
        "    PromptTemplate,  # Template for formatting prompts with dynamic variables\n",
        "    SystemMessagePromptTemplate,  # Represents a system-level instruction to the model\n",
        "    HumanMessagePromptTemplate,  # Represents a human-level input for the model\n",
        "    ChatPromptTemplate,  # Combines multiple prompt components into a unified chat prompt\n",
        ")\n",
        "\n",
        "# Define the system prompt template as a string with placeholders for dynamic content\n",
        "review_template_str = \"\"\"Your job is to use patient reviews to answer questions about their experience at a hospital.\n",
        "Use the following context to answer questions.\n",
        "Be as detailed as possible, but don't make up any information that's not from the context.\n",
        "If you don't know an answer, say you don't know.\n",
        "\n",
        "{context}\n",
        "\"\"\"\n",
        "\n",
        "# Create a system-level message prompt template for the chatbot\n",
        "review_system_prompt = SystemMessagePromptTemplate(\n",
        "    prompt=PromptTemplate(\n",
        "        input_variables=[\"context\"],  # Placeholder for the \"context\" (e.g., patient reviews)\n",
        "        template=review_template_str,  # The instructions and structure of the system prompt\n",
        "    )\n",
        ")\n",
        "\n",
        "# Create a human-level message prompt template for user input\n",
        "review_human_prompt = HumanMessagePromptTemplate(\n",
        "    prompt=PromptTemplate(\n",
        "        input_variables=[\"question\"],  # Placeholder for the \"question\" to be answered\n",
        "        template=\"{question}\",  # A simple template where the \"question\" is dynamically inserted\n",
        "    )\n",
        ")\n",
        "\n",
        "# Combine the system and human prompts into a list of messages\n",
        "messages = [review_system_prompt, review_human_prompt]\n",
        "\n",
        "# Create a chat prompt template that integrates the system and human prompts\n",
        "review_prompt_template = ChatPromptTemplate(\n",
        "    input_variables=[\"context\", \"question\"],  # Define the expected inputs for the template\n",
        "    messages=messages,  # Combine the individual prompt components (system and human)\n",
        ")"
      ],
      "metadata": {
        "id": "uKaKX6pglaVb"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "chat_model = ChatGoogleGenerativeAI(model=\"gemini-2.5-flash\",\n",
        "                                  temperature=0,\n",
        "                                  google_api_key=userdata.get('GOOGLE_API_KEY_2'))"
      ],
      "metadata": {
        "id": "7PFV7mkcjB5W"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
        "from langchain_community.vectorstores import Chroma\n",
        "\n",
        "# Specify the embedding function to use. We define it once to be reused.\n",
        "embedding_function = GoogleGenerativeAIEmbeddings(\n",
        "    model=\"models/gemini-embedding-001\",  # Choose the specific embedding model provided by Google.\n",
        "    google_api_key=userdata.get('GOOGLE_API_KEY_2')  # Securely fetch the Google API key.\n",
        ")\n",
        "\n",
        "reviews_vector_db = Chroma(\n",
        "    persist_directory=\"chroma_data\",\n",
        "    embedding_function=embedding_function\n",
        ")"
      ],
      "metadata": {
        "id": "RZrkWCNGDMzh"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing required modules and classes\n",
        "from langchain.document_loaders.csv_loader import CSVLoader\n",
        "import time\n",
        "\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain.prompts import (\n",
        "    PromptTemplate,  # Template for structuring prompts\n",
        "    SystemMessagePromptTemplate,  # System-level instructions for the model\n",
        "    HumanMessagePromptTemplate,  # Human input instructions for the model\n",
        "    ChatPromptTemplate,  # Combines system and human prompts into a single chat prompt\n",
        ")\n",
        "from langchain_core.output_parsers import StrOutputParser  # Parses the model's output into a clean string\n",
        "from langchain_community.vectorstores import Chroma  # Vector database for efficient similarity searches\n",
        "from langchain_google_genai import GoogleGenerativeAIEmbeddings  # Converts text to embeddings using Google's API\n",
        "from langchain.schema.runnable import RunnablePassthrough  # Allows passing inputs through unchanged in a pipeline\n",
        "\n",
        "# Path to the persistent Chroma vector database\n",
        "REVIEWS_CSV_PATH = \"reviews.csv\"\n",
        "\n",
        "# Define a constant variable for the directory where the Chroma vector database will be stored.\n",
        "REVIEWS_CHROMA_PATH = \"chroma_data\"\n",
        "\n",
        "\n",
        "# Create an instance of the CSVLoader.\n",
        "loader = CSVLoader(\n",
        "    file_path=REVIEWS_CSV_PATH,  # Specify the path to the CSV file to be loaded.\n",
        "    source_column=\"review\"       # Specify the name of the column that contains the main text content.\n",
        ")\n",
        "\n",
        "# Call the .load() method on the loader instance.\n",
        "# This reads the specified column from the CSV file and loads the content into a list of Document objects.\n",
        "reviews = loader.load()\n",
        "\n",
        "# Specify the embedding function to use. We define it once to be reused.\n",
        "embedding_function = GoogleGenerativeAIEmbeddings(\n",
        "    model=\"models/gemini-embedding-004\",  # Choose the specific embedding model provided by Google.\n",
        "    google_api_key=userdata.get('GOOGLE_API_KEY_2')  # Securely fetch the Google API key.\n",
        ")\n",
        "\n",
        "# Set the size of each batch to process.\n",
        "batch_size = 20\n",
        "# Calculate the total number of batches.\n",
        "num_batches = (len(reviews) - 1) // batch_size + 1\n",
        "reviews_vector_db = None\n",
        "\n",
        "# Loop through the documents in batches to avoid hitting the API's rate limit.\n",
        "for i in range(0, len(reviews), batch_size):\n",
        "    # Get the current batch of documents.\n",
        "    batch_docs = reviews[i:i + batch_size]\n",
        "    current_batch_num = i // batch_size + 1\n",
        "\n",
        "    print(f\"Processing batch {current_batch_num}/{num_batches}...\")\n",
        "\n",
        "    if i == 0:\n",
        "        # For the first batch, create a new Chroma vector database.\n",
        "        # The `from_documents` method handles the entire process of embedding and storing the data.\n",
        "        reviews_vector_db = Chroma.from_documents(\n",
        "            documents=batch_docs,  # Pass the list of Document objects that need to be embedded.\n",
        "            embedding=embedding_function,\n",
        "            # Specify the directory on the disk where the vector database will be saved.\n",
        "            # This makes the database persistent, so we can load it directly in the future.\n",
        "            persist_directory=REVIEWS_CHROMA_PATH\n",
        "        )\n",
        "    else:\n",
        "        # For subsequent batches, add the documents to the existing database.\n",
        "        reviews_vector_db.add_documents(documents=batch_docs)\n",
        "\n",
        "    # Pause the script for 60 seconds after each batch to respect the per-minute rate limit.\n",
        "    print(f\"Batch {current_batch_num} processed. Waiting for 30 seconds...\")\n",
        "    time.sleep(30)\n",
        "\n",
        "print(\"Vector database created successfully and saved to the specified directory.\")"
      ],
      "metadata": {
        "id": "5CTUzRoNlvDY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.schema.runnable import RunnablePassthrough\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "# Create a retriever to fetch the top 10 most relevant reviews based on a query\n",
        "reviews_retriever = reviews_vector_db.as_retriever(k=10)\n",
        "# The `as_retriever` method converts the database into a retriever.\n",
        "# `k=10` specifies that the retriever should return the top 10 most relevant documents for a query.\n",
        "\n",
        "# Create a chain for querying and generating responses\n",
        "review_chain = (\n",
        "    {\"context\": reviews_retriever, \"question\": RunnablePassthrough()}\n",
        "    # Step 1: Retrieves relevant reviews (`context`) and passes the `question` unchanged\n",
        "    | review_prompt_template\n",
        "    # Step 2: Formats the retrieved reviews and the user's question into a structured prompt\n",
        "    | chat_model\n",
        "    # Step 3: Sends the prompt to the OpenAI chat model to generate a response\n",
        "    | StrOutputParser()\n",
        "    # Step 4: Parses the model's raw output into a clean string format for easier use\n",
        ")"
      ],
      "metadata": {
        "id": "PV6lQyYDl7WD"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# question = \"\"\"Has anyone complained about communication with the hospital staff?\"\"\"\n",
        "# review_chain.invoke(question)"
      ],
      "metadata": {
        "id": "VSDO_5aymCIr"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Adding a UI"
      ],
      "metadata": {
        "id": "3J_-TawemeDp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gradio"
      ],
      "metadata": {
        "id": "yL-EnjFRmdrb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def respond_to_user_question(question: str, history: list) -> str:\n",
        "    \"\"\"\n",
        "    Respond to a user's question using the review_chain.\n",
        "    \"\"\"\n",
        "    return review_chain.invoke(question)"
      ],
      "metadata": {
        "id": "BnkjE9zXmgqT"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# respond_to_user_question(\"Has anyone complained about communication with the hospital staff?\", [])"
      ],
      "metadata": {
        "id": "Po1hiTd6mko-"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "# Create the Gradio ChatInterface\n",
        "interface = gr.ChatInterface(fn=respond_to_user_question, title=\"Review Helper Bot\")\n",
        "\n",
        "# Launch the Gradio app\n",
        "interface.launch(debug=True)"
      ],
      "metadata": {
        "id": "3vBIk8T_mkln",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 715
        },
        "outputId": "fb047bbb-473b-4286-a10a-94ea41b7447e"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/gradio/chat_interface.py:348: UserWarning: The 'tuples' format for chatbot messages is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style 'role' and 'content' keys.\n",
            "  self.chatbot = Chatbot(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "It looks like you are running Gradio on a hosted Jupyter notebook, which requires `share=True`. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "* Running on public URL: https://6685db31ab6ec72011.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://6685db31ab6ec72011.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Keyboard interruption in main thread... closing server.\n",
            "Killing tunnel 127.0.0.1:7860 <> https://6685db31ab6ec72011.gradio.live\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    }
  ]
}